{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f4e750a-19df-4434-aa6f-d0c8597f18e9",
   "metadata": {},
   "source": [
    "# RAG-system prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "725c6d00-5fd8-40ff-9dd3-ccff26663df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libs Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import docx\n",
    "import pandas as pd\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from langchain.vectorstores import FAISS\n",
    "import magic\n",
    "import re\n",
    "from io import BytesIO\n",
    "import base64\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "import hashlib\n",
    "import logging\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print('Libs Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ae8e73d-506e-4bb9-9649-d1e55b44d053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY: ...kIA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 09:28:56,969 - INFO - üöÄ –ó–∞–ø—É—Å–∫ RAG-—Å–∏—Å—Ç–µ–º—ã...\n",
      "2025-09-19 09:28:56,970 - INFO - –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...\n",
      "2025-09-19 09:28:56,971 - INFO - ‚úÖ –í—Å–µ –∫–µ—à–∏ –Ω–∞–π–¥–µ–Ω—ã. –ó–∞–≥—Ä—É–∂–∞—é –±–µ–∑ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏—è...\n",
      "2025-09-19 09:28:56,977 - INFO - Loading faiss with AVX2 support.\n",
      "2025-09-19 09:28:57,010 - INFO - Successfully loaded faiss with AVX2 support.\n",
      "2025-09-19 09:28:57,029 - INFO - –ö–µ—à –∑–∞–≥—Ä—É–∂–µ–Ω.\n",
      "2025-09-19 09:28:57,030 - INFO - –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–∞: –í –∫–∞–∫–∏—Ö –∑–æ–Ω–∞—Ö –ø–æ –≤–µ—Å—É —Å–Ω–µ–≥–æ–≤–æ–≥–æ –ø–æ–∫—Ä–æ–≤–∞ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –•–µ—Ä—Å–æ–Ω –∏ –ú–µ–ª–∏—Ç–æ–ø–æ–ª—å?\n",
      "2025-09-19 09:28:57,031 - INFO - –û—Ç–≤–µ—Ç –≤–∑—è—Ç –∏–∑ –∫—ç—à–∞.\n",
      "2025-09-19 09:28:57,031 - INFO - –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–∞: –ö–∞–∫–∏–µ —Ä–µ–≥–∏–æ–Ω—ã –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏ –∏–º–µ—é—Ç –≤—ã—Å–æ—Ç–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç k_h, –ø—Ä–µ–≤—ã—à–∞—é—â–∏–π 2?\n",
      "2025-09-19 09:28:57,032 - INFO - –û—Ç–≤–µ—Ç –≤–∑—è—Ç –∏–∑ –∫—ç—à–∞.\n",
      "2025-09-19 09:28:57,033 - INFO - –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–∞: –ö–∞–∫–æ–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –ø–æ –Ω–∞–≥—Ä—É–∑–∫–µ –¥–ª—è –º–µ—Ç–∞–ª–ª–∏—á–µ—Å–∫–∏—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫–∏—Ü–π?\n",
      "2025-09-19 09:28:57,034 - INFO - –û—Ç–≤–µ—Ç –≤–∑—è—Ç –∏–∑ –∫—ç—à–∞.\n",
      "2025-09-19 09:28:57,035 - INFO - –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–∞: –ö–∞–∫ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–π –≤–µ—Ç—Ä–æ–≤–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏ w?\n",
      "2025-09-19 09:28:57,036 - INFO - –û—Ç–≤–µ—Ç –≤–∑—è—Ç –∏–∑ –∫—ç—à–∞.\n",
      "2025-09-19 09:28:57,036 - INFO - –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–∞: –ß—Ç–æ —Ç–∞–∫–æ–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –ø–æ –Ω–∞–≥—Ä—É–∑–∫–µ?\n",
      "2025-09-19 09:28:57,037 - INFO - –û—Ç–≤–µ—Ç –≤–∑—è—Ç –∏–∑ –∫—ç—à–∞.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================================================================\n",
      "*‚ùì* –í–æ–ø—Ä–æ—Å: –í –∫–∞–∫–∏—Ö –∑–æ–Ω–∞—Ö –ø–æ –≤–µ—Å—É —Å–Ω–µ–≥–æ–≤–æ–≥–æ –ø–æ–∫—Ä–æ–≤–∞ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –•–µ—Ä—Å–æ–Ω –∏ –ú–µ–ª–∏—Ç–æ–ø–æ–ª—å?\n",
      "-----------------------------------------------------------------------------\n",
      "–•–µ—Ä—Å–æ–Ω –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –∑–æ–Ω–µ I, –∞ –ú–µ–ª–∏—Ç–æ–ø–æ–ª—å –≤ –∑–æ–Ω–µ II –ø–æ –≤–µ—Å—É —Å–Ω–µ–≥–æ–≤–æ–≥–æ –ø–æ–∫—Ä–æ–≤–∞. –ò—Å—Ç–æ—á–Ω–∏–∫: –ö–ê–†–¢–ê 1, –í.\n",
      "\n",
      "–ò—Å—Ç–æ—á–Ω–∏–∫–∏:\n",
      "- –°–ü 20.13330.2016 2024-09-05.docx: –ö–ê–†–¢–ê 1, –≤. –†–ê–ô–û–ù–ò–†–û–í–ê–ù–ò–ï –¢–ï–†–†–ò–¢–û–†–ò–ò –î–û–ù–ï–¶–ö–û–ô –ù–ê–†–û–î–ù–û–ô –†–ï–°–ü–£–ë–õ–ò–ö–ò, –õ–£–ì–ê–ù–°–ö–û–ô –ù–ê–†–û–î–ù–û–ô –†–ï–°–ü–£–ë–õ–ò–ö–ò, –ó–ê–ü–û–†–û–ñ–°–ö–û–ô –û–ë–õ–ê–°–¢–ò, –•–ï–†–°–û–ù–°–ö–û–ô –û–ë–õ–ê–°–¢–ò –ü–û –í–ï–°–£ –°–ù–ï–ì–û–í–û–ì–û –ü–û–ö–†–û–í–ê (–î–û–ü–û–õ–ù–ï–ù–ò–ï –ö –ö–ê–†–¢–ï 1. –†–ê–ô–û–ù–ò–†–û–í–ê–ù–ò–ï –¢–ï–†–†–ò–¢–û–†–ò–ò –†–û–°–°–ò–ô–°–ö–û–ô –§–ï–î–ï–†–ê–¶–ò–ò –ü–û –í–ï–°–£ –°–ù–ï–ì–û–í–û–ì–û –ü–û–ö–†–û–í–ê) (image, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: 0.873); –ö–ê–†–¢–ê 1, –±. –†–ê–ô–û–ù–ò–†–û–í–ê–ù–ò–ï –¢–ï–†–†–ò–¢–û–†–ò–ò –†–ï–°–ü–£–ë–õ–ò–ö–ò –ö–†–´–ú –ü–û –í–ï–°–£ –°–ù–ï–ì–û–í–û–ì–û –ü–û–ö–†–û–í–ê (image, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: 0.868)\n",
      "=============================================================================\n",
      "\n",
      "=============================================================================\n",
      "*‚ùì* –í–æ–ø—Ä–æ—Å: –ö–∞–∫–∏–µ —Ä–µ–≥–∏–æ–Ω—ã –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏ –∏–º–µ—é—Ç –≤—ã—Å–æ—Ç–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç k_h, –ø—Ä–µ–≤—ã—à–∞—é—â–∏–π 2?\n",
      "-----------------------------------------------------------------------------\n",
      "–†–µ–≥–∏–æ–Ω—ã –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏ —Å –≤—ã—Å–æ—Ç–Ω—ã–º –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–º \\( k_h \\), –ø—Ä–µ–≤—ã—à–∞—é—â–∏–º 2, –≤–∫–ª—é—á–∞—é—Ç:\n",
      "\n",
      "1. –ö—Ä–∞—Å–Ω–æ–¥–∞—Ä—Å–∫–∏–π –∫—Ä–∞–π (–ê–¥–ª–µ—Ä—Å–∫–∏–π —Ä–∞–π–æ–Ω) - \\( k_h = 2,15 \\)\n",
      "2. –†–µ—Å–ø—É–±–ª–∏–∫–∞ –ê–¥—ã–≥–µ—è (–ê–¥—ã–≥–µ—è) - \\( k_h = 2,15 \\)\n",
      "3. –ö—Ä–∞—Å–Ω–æ—è—Ä—Å–∫–∏–π –∫—Ä–∞–π (–ö–µ–º–µ—Ä–æ–≤—Å–∫–∞—è –æ–±–ª–∞—Å—Ç—å - –ö—É–∑–±–∞—Å—Å, –ö—É–∑–Ω–µ—Ü–∫–∏–π –ê–ª–∞—Ç–∞—É, –ì–æ—Ä–Ω–∞—è –®–æ—Ä–∏—è) - \\( k_h = 2,25 \\)\n",
      "4. –ö—Ä–∞—Å–Ω–æ—è—Ä—Å–∫–∏–π –∫—Ä–∞–π (–°–∞—è–Ω—Å–∫–∏–π —Ö—Ä., –ö—É—Ä—Ç—É—à–∏–±–∏–Ω—Å–∫–∏–π —Ö—Ä.) - \\( k_h = 3,15 \\)\n",
      "5. –ù–æ—Ä–∏–ª—å—Å–∫–∏–π –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã–π —Ä–∞–π–æ–Ω (–¥–æ 20 –∫–º –∫ —é–≥—É –æ—Ç –≥. –ù–æ—Ä–∏–ª—å—Å–∫–∞) - \\( k_h = 2,5 \\)\n",
      "6. –ù–æ—Ä–∏–ª—å—Å–∫–∏–π –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã–π —Ä–∞–π–æ–Ω (–∫ —Å–µ–≤–µ—Ä–æ-–≤–æ—Å—Ç–æ–∫—É –æ—Ç –≥. –ù–æ—Ä–∏–ª—å—Å–∫–∞) - \\( k_h = 3,75 \\)\n",
      "7. –†–µ—Å–ø—É–±–ª–∏–∫–∞ –ë—É—Ä—è—Ç–∏—è (–ë–∞–π–∫–∞–ª—å—Å–∫–∏–π —Ö—Ä.) - \\( k_h = 2,3 \\)\n",
      "\n",
      "–ò—Å—Ç–æ—á–Ω–∏–∫: –¢–∞–±–ª–∏—Ü–∞ 49.\n",
      "\n",
      "–ò—Å—Ç–æ—á–Ω–∏–∫–∏:\n",
      "- –°–ü 20.13330.2016 2024-09-05.docx: –¢–∞–±–ª–∏—Ü–∞ 49 (table, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: 0.964); –ö–ê–†–¢–ê 1. –†–ê–ô–û–ù–ò–†–û–í–ê–ù–ò–ï –¢–ï–†–†–ò–¢–û–†–ò–ò –†–û–°–°–ò–ô–°–ö–û–ô –§–ï–î–ï–†–ê–¶–ò–ò –ü–û –í–ï–°–£ –°–ù–ï–ì–û–í–û–ì–û –ü–û–ö–†–û–í–ê (formula, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: 0.936); –¢–∞–±–ª–∏—Ü–∞ 11.2 (formula, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: 0.930)\n",
      "=============================================================================\n",
      "\n",
      "=============================================================================\n",
      "*‚ùì* –í–æ–ø—Ä–æ—Å: –ö–∞–∫–æ–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –ø–æ –Ω–∞–≥—Ä—É–∑–∫–µ –¥–ª—è –º–µ—Ç–∞–ª–ª–∏—á–µ—Å–∫–∏—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫–∏—Ü–π?\n",
      "-----------------------------------------------------------------------------\n",
      "–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –ø–æ –Ω–∞–≥—Ä—É–∑–∫–µ –¥–ª—è –º–µ—Ç–∞–ª–ª–∏—á–µ—Å–∫–∏—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–π —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç 1,05 (–¢–∞–±–ª–∏—Ü–∞ 1).\n",
      "\n",
      "–ò—Å—Ç–æ—á–Ω–∏–∫–∏:\n",
      "- –°–ü 20.13330.2016 2024-09-05.docx: –¢–∞–±–ª–∏—Ü–∞ 1 (table, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: 0.980); –¢–∞–±–ª–∏—Ü–∞ 3 (table, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: 0.964); –¢–∞–±–ª–∏—Ü–∞ 26 (table, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: 0.959); –î–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —É—á–∞—Å—Ç–∫–æ–≤ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–π (—Ä–∏—Å—É–Ω–æ–∫ –í.1) –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç cx –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –ø–æ —Ç–∞–±–ª–∏—Ü–µ –í.1; (image, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: 0.886)\n",
      "=============================================================================\n",
      "\n",
      "=============================================================================\n",
      "*‚ùì* –í–æ–ø—Ä–æ—Å: –ö–∞–∫ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–π –≤–µ—Ç—Ä–æ–≤–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏ w?\n",
      "-----------------------------------------------------------------------------\n",
      "–ù–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–π –≤–µ—Ç—Ä–æ–≤–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏ w –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –∫–∞–∫ —Å—É–º–º–∞ —Å—Ä–µ–¥–Ω–µ–π —Å–æ—Å—Ç–∞–≤–ª—è—é—â–µ–π –≤–µ—Ç—Ä–æ–≤–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏ wm –∏ –ø—É–ª—å—Å–∞—Ü–∏–æ–Ω–Ω–æ–π —Å–æ—Å—Ç–∞–≤–ª—è—é—â–µ–π wg, –ø–æ —Ñ–æ—Ä–º—É–ª–µ: \n",
      "\n",
      "w = wm + wg. (11.1)\n",
      "\n",
      "–ò—Å—Ç–æ—á–Ω–∏–∫: –°–ü 20.13330.2016, —Ä–∞–∑–¥–µ–ª 11.1.2.\n",
      "\n",
      "–ò—Å—Ç–æ—á–Ω–∏–∫–∏:\n",
      "- –ù–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–ª–∏ —Ñ–æ—Ä–º—É–ª\n",
      "=============================================================================\n",
      "\n",
      "=============================================================================\n",
      "*‚ùì* –í–æ–ø—Ä–æ—Å: –ß—Ç–æ —Ç–∞–∫–æ–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –ø–æ –Ω–∞–≥—Ä—É–∑–∫–µ?\n",
      "-----------------------------------------------------------------------------\n",
      "–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –ø–æ –Ω–∞–≥—Ä—É–∑–∫–µ ‚Äî —ç—Ç–æ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç, —É—á–∏—Ç—ã–≤–∞—é—â–∏–π –≤ —É—Å–ª–æ–≤–∏—è—Ö –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏ —Å–æ–æ—Ä—É–∂–µ–Ω–∏–π –≤–æ–∑–º–æ–∂–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –Ω–∞–≥—Ä—É–∑–æ–∫ –≤ –Ω–µ–±–ª–∞–≥–æ–ø—Ä–∏—è—Ç–Ω—É—é (–±–æ–ª—å—à—É—é –∏–ª–∏ –º–µ–Ω—å—à—É—é) —Å—Ç–æ—Ä–æ–Ω—É –æ—Ç –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (–°–ü 20.13330.2016, –ø. 3.2).\n",
      "\n",
      "–ò—Å—Ç–æ—á–Ω–∏–∫–∏:\n",
      "- –°–ü 20.13330.2016 2024-09-05.docx: –¢–∞–±–ª–∏—Ü–∞ 3 (table, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: 0.951); –¢–∞–±–ª–∏—Ü–∞ 1 (table, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: 0.951); –†–∏—Å—É–Ω–æ–∫¬†11.1¬†-¬†–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã¬†–¥–∏–Ω–∞–º–∏—á–Ω–æ—Å—Ç–∏\n",
      "(–≤¬†—Ä–µ–¥.¬†–ò–∑–º–µ–Ω–µ–Ω–∏—è¬†N¬†1,¬†—É—Ç–≤.¬†–ü—Ä–∏–∫–∞–∑–æ–º\n",
      "–ú–∏–Ω—Å—Ç—Ä–æ—è¬†–†–æ—Å—Å–∏–∏¬†–æ—Ç¬†05.07.2018¬†N¬†402/–ø—Ä) (formula, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: 0.918)\n",
      "=============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load .env params\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "print(f\"OPENAI_API_KEY: ...{OPENAI_API_KEY[-3:]}\")\n",
    "\n",
    "\n",
    "# --- Settings ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'  # 4 Linux/macOS\n",
    "\n",
    "\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "llm = ChatOpenAI(model=MODEL_NAME, temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Simple cache on the basis of files\n",
    "INDEX_PATH = \"faiss_index\"\n",
    "IMAGE_CACHE_FILE = \"image_cache.json\"\n",
    "RAG_CACHE_FILE = \"rag_cache.json\"\n",
    "IMAGES_DIR = \"images\"\n",
    "CHUNKS_LOG_FILE = \"chunks.json\"\n",
    "LINKS_DICT_FILE = \"links.json\"\n",
    "\n",
    "FORMULA_AREA_THRESHOLD = 50000\n",
    "MAX_TOKENS_PER_BATCH = 250000\n",
    "\n",
    "\n",
    "def is_image_file(file_path: str) -> bool:\n",
    "    \"\"\"\n",
    "    –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mime = magic.Magic(mime=True)\n",
    "        mime_type = mime.from_file(file_path)\n",
    "        return mime_type.startswith('image')\n",
    "    except Exception as e:\n",
    "        logger.error(f\"is_image_file Exception: checking MIME for {file_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def convert_to_png(image_data: bytes, output_path: str) -> bool:\n",
    "    \"\"\"\n",
    "    –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –±–∞–π—Ç–æ–≤–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ PNG (—Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π WMF/EMF).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        try:\n",
    "            img = Image.open(BytesIO(image_data))\n",
    "            img.save(output_path, 'PNG')\n",
    "            return True\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            from wand.image import Image as WandImage\n",
    "            with WandImage(blob=image_data) as img:\n",
    "                img.format = 'png'\n",
    "                img.save(filename=output_path)\n",
    "            return True\n",
    "        except ImportError:\n",
    "            logger.warning(\"Wand is not installed.\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"convert_to_png Exception: Wand for {output_path}: {e}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"convert_to_png Exception: convert to PNG {output_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def image_to_base64(img_path: str) -> Optional[str]:\n",
    "    logger.info(f\"–ü–æ–ø—ã—Ç–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {img_path}\")\n",
    "    try:\n",
    "        if not os.path.exists(img_path):\n",
    "            logger.warning(f\"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ: {img_path}\")\n",
    "            return None\n",
    "        img = Image.open(img_path)\n",
    "        buffered = BytesIO()\n",
    "        img.save(buffered, format=\"PNG\")\n",
    "        logger.info(f\"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {img_path} —É—Å–ø–µ—à–Ω–æ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –≤ base64\")\n",
    "        return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "    except Exception as e:\n",
    "        logger.error(f\"image_to_base64 Exception: {img_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def ocr_image(img_path: str, lang='eng+rus+equ') -> str:\n",
    "    try:\n",
    "        if not os.path.exists(img_path):\n",
    "            return \"\"\n",
    "        img = Image.open(img_path)\n",
    "        text = pytesseract.image_to_string(img, lang=lang)\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"ocr_image Exception: {img_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_images_from_zip(doc_path: str, images_dir: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ .docx like ZIP.\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    os.makedirs(images_dir, exist_ok=True)\n",
    "    try:\n",
    "        with zipfile.ZipFile(doc_path, 'r') as zip_ref:\n",
    "            for file_name in zip_ref.namelist():\n",
    "                if file_name.startswith('word/media/') and any(ext in file_name.lower() for ext in ['jpg', 'jpeg', 'png', 'bmp', 'gif', 'wmf']):\n",
    "                    try:\n",
    "                        image_data = zip_ref.read(file_name)\n",
    "                        hash_id = hashlib.md5(image_data).hexdigest()\n",
    "                        image_path = os.path.join(images_dir, f\"img_{hash_id[:8]}.png\")\n",
    "                        if not os.path.exists(image_path):\n",
    "                            if convert_to_png(image_data, image_path) and is_image_file(image_path):\n",
    "                                images.append(image_path)\n",
    "                        else:\n",
    "                            images.append(image_path)\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"extract_images_from_zip Error: processing {file_name}: {e}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"extract_images_from_zip Exception: {e}\")\n",
    "    return images\n",
    "\n",
    "def extract_links(text: str) -> Dict[str, None]:\n",
    "    matches = \\\n",
    "    re.findall(\n",
    "        r'(—Ä–∏—Å—É–Ω–æ–∫|—Ç–∞–±–ª–∏—Ü–∞|—Ñ–æ—Ä–º—É–ª–∞|–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ|–∫–∞—Ä—Ç–∞)\\s+([\\d\\w–∞-—è–ê-–Øa-zA-Z\\.,\\s\\-/]+?)(?=\\s*[\\(\\.\\,\\;]|\\s*$)', \n",
    "        text, \n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    return {f\"{m[0].lower()} {m[1].strip('.')}\": None for m in matches}\n",
    "\n",
    "def extract_from_docx(\n",
    "    doc_path: str, \n",
    "    images_dir: str = IMAGES_DIR,\n",
    "    images_cache_file: str = IMAGE_CACHE_FILE,\n",
    "    chunks_log_file: str = CHUNKS_LOG_FILE,\n",
    "    links_dict_file: str = LINKS_DICT_FILE) -> Tuple[List[LangchainDocument], Dict[str, str], bool]:\n",
    "    \n",
    "    if not os.path.exists(doc_path):\n",
    "        raise FileNotFoundError(f\"–§–∞–π–ª {doc_path} –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n",
    "    documents = []\n",
    "    links = {}\n",
    "    current_section = \"\"\n",
    "    current_subsection = \"\"\n",
    "    image_index = 0\n",
    "    table_index = 0\n",
    "    fallback_used = False\n",
    "    doc_filename = os.path.basename(doc_path)\n",
    "    os.makedirs(images_dir, exist_ok=True)\n",
    "    image_cache = {}\n",
    "    zip_image_index = {}\n",
    "    zip_images = extract_images_from_zip(doc_path, images_dir)\n",
    "    \n",
    "    for img in zip_images:\n",
    "        try:\n",
    "            with open(img, 'rb') as f:\n",
    "                image_data = f.read()\n",
    "                hash_id = hashlib.md5(image_data).hexdigest()\n",
    "                zip_image_index[hash_id] = img\n",
    "        except Exception as e:\n",
    "            logger.error(f\"extract_from_docx Exception: hashing {img}: {e}\")\n",
    "    try:\n",
    "        doc = docx.Document(doc_path)\n",
    "        pending_header = None\n",
    "        section_level = 0  # 0: section, 1: subsection\n",
    "        for para in doc.paragraphs:\n",
    "            text = para.text.strip()\n",
    "            style_name = para.style.name if para.style else \"\"\n",
    "            \n",
    "            if style_name.startswith(\"Heading\") or (text.isupper() and len(text) < 120 and re.match(r'^[–ê-–ØA-Z0-9\\s\\-\\.\\(\\)]{5,}$', text)):\n",
    "                if \"–ë–ò–ë–õ–ò–û–ì–†–ê–§–ò–Ø\" in text.upper():\n",
    "                    current_section = \"–ë–ò–ë–õ–ò–û–ì–†–ê–§–ò–Ø\"\n",
    "                    current_subsection = \"\"\n",
    "                elif re.match(r'^\\d+\\.\\d+', text):  # –ü–æ–¥—Ä–∞–∑–¥–µ–ª\n",
    "                    current_subsection = text.strip()\n",
    "                    section_level = 1\n",
    "                else:\n",
    "                    current_section = text.strip()\n",
    "                    current_subsection = \"\"  # Reset\n",
    "                    section_level = 0\n",
    "                    \n",
    "            if any(k in style_name.lower() for k in [\"caption\", \"–ø–æ–¥–ø–∏—Å—å\"]) or \\\n",
    "               re.search(r'(—Ä–∏—Å—É–Ω–æ–∫|—Ç–∞–±–ª–∏—Ü–∞|—Ñ–æ—Ä–º—É–ª–∞|–∫–∞—Ä—Ç–∞|–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ)\\s+([\\d\\w\\.,\\s\\-]+)', text, re.IGNORECASE):\n",
    "                if len(text) <= 500:\n",
    "                    pending_header = text.strip()\n",
    "                    \n",
    "            if text and not style_name.startswith(\"Heading\") and \"caption\" not in style_name.lower():\n",
    "                header_context = f\"{current_section} ‚Üí {current_subsection}\".strip(\" ‚Üí \")\n",
    "                content = f\"{header_context}\\n{text}\" if header_context else text\n",
    "                local_links = extract_links(content)\n",
    "                keywords = [current_section.lower(), current_subsection.lower()] + list(local_links.keys())\n",
    "                keywords += re.findall(r'\\b(–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç|–∑–æ–Ω–∞|—Ä–∞–π–æ–Ω|—Ä–µ–≥–∏–æ–Ω|–æ–±–ª–∞—Å—Ç—å|–∫—Ä–∞–π|—Ä–µ—Å–ø—É–±–ª–∏–∫–∞)\\b', content.lower(), re.IGNORECASE)\n",
    "                doc_item = LangchainDocument(\n",
    "                    page_content=content,\n",
    "                    metadata={\n",
    "                        \"source\": f\"para_{len(documents)}\",\n",
    "                        \"type\": \"text\",\n",
    "                        \"links\": local_links,\n",
    "                        \"filename\": doc_filename,\n",
    "                        \"header\": header_context,\n",
    "                        \"section\": current_section,\n",
    "                        \"subsection\": current_subsection,\n",
    "                        \"keywords\": list(set(keywords))\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc_item)\n",
    "                \n",
    "            for run in para.runs:\n",
    "                for blip in run._element.xpath('.//a:blip'):\n",
    "                    rel_id = blip.embed\n",
    "                    if rel_id in doc.part.rels:\n",
    "                        try:\n",
    "                            rel = doc.part.rels[rel_id]\n",
    "                            if \"image\" in rel.reltype:\n",
    "                                image_data = rel.target_part.blob\n",
    "                                hash_id = hashlib.md5(image_data).hexdigest()\n",
    "                                \n",
    "                                if hash_id in zip_image_index:\n",
    "                                    image_path = zip_image_index[hash_id]\n",
    "                                    logger.warning(f\"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –¥—É–±–ª–∏—Ä—É—é—â–µ–µ—Å—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å —Ö—ç—à–µ–º {hash_id}, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è: {image_path}\")\n",
    "                                else:\n",
    "                                    image_index += 1\n",
    "                                    image_path = os.path.join(images_dir, f\"image_{image_index}.png\")\n",
    "                                    if convert_to_png(image_data, image_path) and is_image_file(image_path):\n",
    "                                        zip_image_index[hash_id] = image_path\n",
    "                                    else:\n",
    "                                        continue\n",
    "                                        \n",
    "                                img = Image.open(image_path)\n",
    "                                width, height = img.size\n",
    "                                area = width * height\n",
    "                                final_header = pending_header or current_subsection or f\"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {image_index}\"\n",
    "                                full_header = f\"{current_section}\\n{final_header}\"\n",
    "                                ocr_text = \"\"\n",
    "                                if area < FORMULA_AREA_THRESHOLD:\n",
    "                                    ocr_text = ocr_image(image_path)\n",
    "                                    page_content = f\"{final_header}\\n{ocr_text}\"\n",
    "                                    doc_type = \"formula\"\n",
    "                                    link_key_match = re.search(r'(—Ñ–æ—Ä–º—É–ª–∞|—Ä–∏—Å—É–Ω–æ–∫)\\s+([\\d\\w\\.,\\s\\-]+)', final_header, re.IGNORECASE)\n",
    "                                    link_key = f\"—Ñ–æ—Ä–º—É–ª–∞ {link_key_match.group(2)}\" if link_key_match else f\"—Ñ–æ—Ä–º—É–ª–∞ {image_index}\"\n",
    "                                    links[link_key] = ocr_text\n",
    "                                else:\n",
    "                                    page_content = final_header  # –ë–µ–∑ OCR\n",
    "                                    doc_type = \"image\"\n",
    "                                    link_key_match = re.search(r'(—Ä–∏—Å—É–Ω–æ–∫|–∫–∞—Ä—Ç–∞|–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ)\\s+([\\d\\w\\.,\\s\\-]+)', final_header, re.IGNORECASE)\n",
    "                                    link_key = f\"{link_key_match.group(1).lower()} {link_key_match.group(2)}\" if link_key_match else f\"—Ä–∏—Å—É–Ω–æ–∫ {image_index}\"\n",
    "                                    links[link_key] = image_path\n",
    "                                    \n",
    "                                if not any(d.metadata[\"source\"] == f\"img_{image_index}\" for d in documents):\n",
    "                                    keywords = [current_section.lower(), final_header.lower(), link_key.lower()]\n",
    "                                    keywords += re.findall(r'\\b(–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç|–∑–æ–Ω–∞|—Ä–∞–π–æ–Ω|—Ä–µ–≥–∏–æ–Ω|–æ–±–ª–∞—Å—Ç—å|–∫—Ä–∞–π|—Ä–µ—Å–ø—É–±–ª–∏–∫–∞|–∫–∞—Ä—Ç–∞)\\b', final_header.lower(), re.IGNORECASE)  # –ò–∑ header, –Ω–µ OCR\n",
    "                                    img_doc = LangchainDocument(\n",
    "                                        page_content=page_content,\n",
    "                                        metadata={\n",
    "                                            \"source\": f\"img_{image_index}\",\n",
    "                                            \"type\": doc_type,\n",
    "                                            \"links\": {link_key: links[link_key]},\n",
    "                                            \"filename\": doc_filename,\n",
    "                                            \"header\": final_header,\n",
    "                                            \"section\": current_section,\n",
    "                                            \"subsection\": current_subsection,\n",
    "                                            \"keywords\": list(set(keywords))\n",
    "                                        }\n",
    "                                    )\n",
    "                                    documents.append(img_doc)\n",
    "                                    \n",
    "                                if image_path not in image_cache:\n",
    "                                    image_cache[image_path] = {\n",
    "                                        \"ocr\": ocr_text,\n",
    "                                        \"size\": (width, height),\n",
    "                                        \"header\": full_header,\n",
    "                                        \"type\": doc_type\n",
    "                                    }\n",
    "                                    \n",
    "                                if pending_header:\n",
    "                                    pending_header = None\n",
    "                        except Exception as e:\n",
    "                            logger.error(f\"extract_from_docx Exception: image processing: {e}\")\n",
    "        for table in doc.tables:\n",
    "            table_index += 1\n",
    "            header = pending_header or f\"–¢–∞–±–ª–∏—Ü–∞ {table_index}\"\n",
    "            # New: Check first row for title\n",
    "            first_row_text = \" \".join(cell.text.strip() for cell in table.rows[0].cells).lower()\n",
    "            \n",
    "            if \"—Ç–∞–±–ª–∏—Ü–∞\" in first_row_text or re.search(r'—Ç–∞–±–ª–∏—Ü–∞\\s+([\\d\\w\\.,\\s\\-/]+)', first_row_text):\n",
    "                header = table.rows[0].cells[0].text.strip()  # Assume title in first cell\n",
    "            if current_subsection and \"–ë–ò–ë–õ–ò–û–ì–†–ê–§–ò–Ø\" not in current_subsection.upper():\n",
    "                header += f\" –≤ {current_subsection}\"\n",
    "            elif current_section and \"–ë–ò–ë–õ–ò–û–ì–†–ê–§–ò–Ø\" not in current_section.upper():\n",
    "                header += f\" –≤ {current_section}\"\n",
    "                \n",
    "            pending_header = None\n",
    "            df = pd.DataFrame([[cell.text.strip() for cell in row.cells] for row in table.rows])\n",
    "            table_md = df.to_markdown(index=False)\n",
    "            match = re.search(r'(—Ç–∞–±–ª–∏—Ü–∞|–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ)\\s+([\\d\\w\\.,\\s\\-/]+)', header, re.IGNORECASE)\n",
    "            link_key = f\"{match.group(1).lower()} {match.group(2).strip('.')}\" if match else f\"—Ç–∞–±–ª–∏—Ü–∞ {table_index}\"\n",
    "            links[link_key] = table_md\n",
    "            table_content = \" \".join([cell.text.strip() for row in table.rows for cell in row.cells]).lower()\n",
    "            table_keywords = re.findall(r'\\b(–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç|–∑–æ–Ω–∞|—Ä–∞–π–æ–Ω|—Ä–µ–≥–∏–æ–Ω|–æ–±–ª–∞—Å—Ç—å|–∫—Ä–∞–π|—Ä–µ—Å–ø—É–±–ª–∏–∫–∞)\\b', table_content, re.IGNORECASE)\n",
    "            table_keywords += [link_key.lower(), current_section.lower(), current_subsection.lower()]\n",
    "            table_doc = LangchainDocument(\n",
    "                page_content=f\"{header}\\n{table_md}\",\n",
    "                metadata={\n",
    "                    \"source\": f\"table_{table_index}\",\n",
    "                    \"type\": \"table\",\n",
    "                    \"links\": {link_key: table_md},\n",
    "                    \"filename\": doc_filename,\n",
    "                    \"header\": header,\n",
    "                    \"section\": current_section,\n",
    "                    \"subsection\": current_subsection,\n",
    "                    \"keywords\": list(set(table_keywords))\n",
    "                }\n",
    "            )\n",
    "            documents.append(table_doc)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"extract_from_docx Exception: python-docx, transition to fallback: {e}\")\n",
    "        fallback_used = True\n",
    "        from docx2txt import process\n",
    "        text = process(doc_path)\n",
    "        chunks = [c.strip() for c in text.split('\\n\\n') if c.strip()]\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            local_links = extract_links(chunk)\n",
    "            doc_item = LangchainDocument(\n",
    "                page_content=chunk,\n",
    "                metadata={\n",
    "                    \"source\": f\"fallback_{i}\",\n",
    "                    \"type\": \"text\",\n",
    "                    \"links\": local_links,\n",
    "                    \"filename\": doc_filename,\n",
    "                    \"header\": \"\",\n",
    "                    \"section\": \"\",\n",
    "                    \"subsection\": \"\",\n",
    "                    \"keywords\": list(local_links.keys())\n",
    "                }\n",
    "            )\n",
    "            documents.append(doc_item)\n",
    "            \n",
    "    with open(images_cache_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(image_cache, f, ensure_ascii=False, indent=2)\n",
    "    logger.info(f\"–ö—ç—à –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {images_cache_file}\")\n",
    "    \n",
    "    with open(links_dict_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(links, f, ensure_ascii=False, indent=2)\n",
    "    logger.info(f\"–°–ª–æ–≤–∞—Ä—å links —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {links_dict_file}\")\n",
    "    \n",
    "    chunks_log = [\n",
    "        {\n",
    "            \"source\": d.metadata[\"source\"],\n",
    "            \"type\": d.metadata[\"type\"],\n",
    "            \"section\": d.metadata.get(\"section\", \"\"),\n",
    "            \"subsection\": d.metadata.get(\"subsection\", \"\"),\n",
    "            \"content\": d.page_content[:200]\n",
    "        } for d in documents\n",
    "    ]\n",
    "    \n",
    "    with open(chunks_log_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(chunks_log, f, ensure_ascii=False, indent=2)\n",
    "    logger.info(f\"–õ–æ–≥ —á–∞–Ω–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {chunks_log_file}\")\n",
    "    \n",
    "    return documents, links, fallback_used\n",
    "\n",
    "def group_by_section(docs: List[LangchainDocument]) -> List[LangchainDocument]:\n",
    "    grouped = defaultdict(list)\n",
    "    for d in docs:\n",
    "        key = (d.metadata.get(\"section\", \"\"), d.metadata.get(\"subsection\", \"\"), d.metadata.get(\"type\", \"text\"))\n",
    "        grouped[key].append(d)\n",
    "    \n",
    "    new_docs = []\n",
    "    for (section, subsection, dtype), group_docs in grouped.items():\n",
    "        content = \"\\n\".join(d.page_content for d in group_docs)\n",
    "        first_metadata = group_docs[0].metadata\n",
    "        all_keywords = set()\n",
    "        all_links = {}  # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º links\n",
    "        all_headers = set()  # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º headers\n",
    "        for d in group_docs:\n",
    "            all_keywords.update(d.metadata.get(\"keywords\", []))\n",
    "            all_links.update(d.metadata.get(\"links\", {}))  # –û–±—ä–µ–¥–∏–Ω—è–µ–º dict links\n",
    "            if d.metadata.get(\"header\"):\n",
    "                all_headers.add(d.metadata.get(\"header\"))  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ headers\n",
    "        \n",
    "        new_docs.append(LangchainDocument(\n",
    "            page_content=content,\n",
    "            metadata={\n",
    "                \"section\": section,\n",
    "                \"subsection\": subsection,\n",
    "                \"type\": dtype,\n",
    "                \"keywords\": list(all_keywords),\n",
    "                \"filename\": first_metadata.get(\"filename\", \"unknown\"),\n",
    "                \"source\": \"; \".join(set(d.metadata.get(\"source\", \"\") for d in group_docs)),\n",
    "                \"links\": all_links,  # –î–æ–±–∞–≤–ª–µ–Ω–æ\n",
    "                \"headers\": list(all_headers)  # –î–æ–±–∞–≤–ª–µ–Ω–æ\n",
    "            }\n",
    "        ))\n",
    "    return new_docs\n",
    "\n",
    "def build_index(documents: List[LangchainDocument], index_path: str = INDEX_PATH) -> FAISS:\n",
    "    if os.path.exists(index_path):\n",
    "        logger.info(\"–ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–≥–æ FAISS –∏–Ω–¥–µ–∫—Å–∞...\")\n",
    "        vectorstore = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "        return vectorstore\n",
    "    \n",
    "    logger.info(\"–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ FAISS –∏–Ω–¥–µ–∫—Å–∞...\")\n",
    "    \n",
    "    text_docs = [d for d in documents if d.metadata[\"type\"] == \"text\"]\n",
    "    other_docs = [d for d in documents if d.metadata[\"type\"] != \"text\"]  # Tables, images, formulas as is\n",
    "    grouped_text = group_by_section(text_docs)\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    chunked_texts = splitter.split_documents(grouped_text)\n",
    "    final_docs = chunked_texts + other_docs  # Add grouped text + original others\n",
    "    \n",
    "    vectorstore = None\n",
    "    batch_size = 200\n",
    "    for i in range(0, len(final_docs), batch_size):\n",
    "        batch = final_docs[i:i + batch_size]\n",
    "        logger.info(f\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {i//batch_size + 1} –∏–∑ {len(final_docs)//batch_size + 1}...\")\n",
    "        try:\n",
    "            if vectorstore is None:\n",
    "                vectorstore = FAISS.from_documents(batch, embeddings)\n",
    "            else:\n",
    "                vectorstore.add_documents(batch)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"build_index Exception: batch processing {i//batch_size + 1}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    vectorstore.save_local(index_path)\n",
    "    logger.info(f\"–ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {index_path}\")\n",
    "    return vectorstore\n",
    "\n",
    "def filter_docs_by_relevance(\n",
    "    docs: List[LangchainDocument], \n",
    "    question: str, \n",
    "    embeddings, \n",
    "    threshold: float = 0.75) -> List[Tuple[LangchainDocument, float]]:\n",
    "    \n",
    "    question_embedding = embeddings.embed_query(question)\n",
    "    filtered = []\n",
    "    \n",
    "    logger.info(f\"–î–æ–∫—É–º–µ–Ω—Ç—ã –ø–µ—Ä–µ–¥ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞ '{question}':\")\n",
    "    \n",
    "    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–ª–æ–≤–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞ (–±–µ–∑ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è, in lower)\n",
    "    question_words = set(re.findall(r'\\b\\w+\\b', question.lower()))\n",
    "    \n",
    "    for doc in docs:\n",
    "        doc_type = doc.metadata.get(\"type\", \"text\")\n",
    "        header = \", \".join(doc.metadata.get(\"headers\", [doc.metadata.get(\"header\", \"\")])).lower()\n",
    "        keywords = set(doc.metadata.get(\"keywords\", []))\n",
    "        \n",
    "        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞\n",
    "        if doc_type == \"image\":\n",
    "            doc_text = f\"{header} {' '.join(keywords)}\"\n",
    "        else:\n",
    "            doc_text = f\"{doc.page_content} {' '.join(keywords)} {header}\"\n",
    "        \n",
    "        try:\n",
    "            doc_embedding = embeddings.embed_query(doc_text)\n",
    "            similarity = cosine_similarity([question_embedding], [doc_embedding])[0][0]\n",
    "            \n",
    "            # –ë–∞–∑–æ–≤—ã–π –ø–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
    "            adj_threshold = 0.7 if doc_type in [\"table\", \"formula\"] else threshold\n",
    "            if doc_type == \"image\":\n",
    "                adj_threshold = 0.5  # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –º–µ–Ω–µ–µ —Å—Ç—Ä–æ–≥–∏–µ\n",
    "            \n",
    "            # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤ –≤–æ–ø—Ä–æ—Å–∞ —Å keywords –∏ header\n",
    "            keyword_overlap = len(question_words.intersection(keywords)) / max(len(question_words), 1)\n",
    "            header_overlap = len(question_words.intersection(set(re.findall(r'\\b\\w+\\b', header)))) / max(len(question_words), 1)\n",
    "            \n",
    "            # Boost –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é\n",
    "            similarity += (keyword_overlap + header_overlap) * 0.1\n",
    "            \n",
    "            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π boost –¥–ª—è —Ç–∞–±–ª–∏—Ü –∏ —Ñ–æ—Ä–º—É–ª, –µ—Å–ª–∏ –≤ –≤–æ–ø—Ä–æ—Å–µ –µ—Å—Ç—å —Å–ª–æ–≤–∞, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –¥–∞–Ω–Ω—ã–º–∏\n",
    "            data_related_words = {\"—Ç–∞–±–ª–∏—Ü–∞\", \"—Ç–∞–±–ª–∏—Ü—ã\", \"–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç\", \"–∑–Ω–∞—á–µ–Ω–∏–µ\", \"–∑–Ω–∞—á–µ–Ω–∏—è\", \"–ø–∞—Ä–∞–º–µ—Ç—Ä\", \"–ø–∞—Ä–∞–º–µ—Ç—Ä—ã\"}\n",
    "            if doc_type in [\"table\", \"formula\"] and question_words.intersection(data_related_words):\n",
    "                similarity += 0.1  # –£—Å–∏–ª–µ–Ω–∏–µ –¥–ª—è —Ç–∞–±–ª–∏—Ü/—Ñ–æ—Ä–º—É–ª –ø—Ä–∏ –≤–æ–ø—Ä–æ—Å–∞—Ö –æ –¥–∞–Ω–Ω—ã—Ö\n",
    "                adj_threshold = 0.65  # –ú–µ–Ω—å—à–∏–π –ø–æ—Ä–æ–≥\n",
    "            \n",
    "            # Boost –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –µ—Å–ª–∏ –≤–æ–ø—Ä–æ—Å —Å–≤—è–∑–∞–Ω —Å –≤–∏–∑—É–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏\n",
    "            visual_words = {\"–∫–∞—Ä—Ç–∞\", \"–∑–æ–Ω–∞\", \"—Ä–∞–π–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ\", \"—Ä–∏—Å—É–Ω–æ–∫\", \"–¥–∏–∞–≥—Ä–∞–º–º–∞\"}\n",
    "            if doc_type == \"image\" and question_words.intersection(visual_words):\n",
    "                similarity += 0.05  # –ú–µ–Ω—å—à–∏–π boost –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
    "                adj_threshold = 0.45  # –ú–µ–Ω—å—à–∏–π –ø–æ—Ä–æ–≥\n",
    "            \n",
    "            # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏\n",
    "            if similarity >= adj_threshold:\n",
    "                filtered.append((doc, similarity))\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"filter_docs_by_relevance Exception: document embedding calculation {doc.metadata.get('source', 'unknown')}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏\n",
    "    filtered = sorted(filtered, key=lambda x: x[1], reverse=True)[:15] # –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–æ 15 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n",
    "    \n",
    "    if not filtered:\n",
    "        logger.warning(\"–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–µ –¥–∞–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –≤–æ–∑–≤—Ä–∞—â–∞—é –ø–µ—Ä–≤—ã–µ 15 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\")\n",
    "        filtered = [(doc, 0.0) for doc in docs[:15]]\n",
    "    \n",
    "    logger.info(f\"–ù–∞–π–¥–µ–Ω–æ {len(filtered)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –ø–æ—Ä–æ–≥–æ–º {threshold}\")\n",
    "    \n",
    "    return filtered\n",
    "    \n",
    "def rag_query(\n",
    "    question: str, \n",
    "    vectorstore: FAISS, \n",
    "    links: Dict[str, str], \n",
    "    cache_file: str = RAG_CACHE_FILE, \n",
    "    fallback_used: bool = False) -> Tuple[str, List]:\n",
    "    \n",
    "    logger.info(f\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–ø—Ä–æ—Å–∞: {question}\")\n",
    "        \n",
    "    if os.path.exists(cache_file):\n",
    "        with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "            rag_cache = json.load(f)\n",
    "    else:\n",
    "        rag_cache = {}\n",
    "        \n",
    "    norm_question = re.sub(r'\\s+', ' ', question.strip().lower())\n",
    "    if norm_question in rag_cache:\n",
    "        logger.info(\"–û—Ç–≤–µ—Ç –≤–∑—è—Ç –∏–∑ –∫—ç—à–∞.\")\n",
    "        return rag_cache[norm_question], []\n",
    "        \n",
    "    retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 30})\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    filtered_docs = filter_docs_by_relevance(docs, question, embeddings)\n",
    "    \n",
    "    logger.info(f\"–û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {len(filtered_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:\")\n",
    "    \n",
    "    context_parts = []\n",
    "    text_parts = []\n",
    "    table_parts = []\n",
    "    image_parts = []\n",
    "    formula_parts = []\n",
    "    image_count = 0\n",
    "    max_images = 5\n",
    "    analyzed_images = set()\n",
    "    image_analysis_results = []\n",
    "    \n",
    "    for doc, score in filtered_docs:\n",
    "        doc_type = doc.metadata.get(\"type\", \"text\")\n",
    "        source = doc.metadata.get(\"source\", \"unknown\")\n",
    "        header = \", \".join(doc.metadata.get(\"headers\", [doc.metadata.get(\"header\", \"\")]))\n",
    "        \n",
    "        if doc_type == \"table\":\n",
    "            content = doc.page_content.strip()\n",
    "            table_parts.append(f\"=== –¢–ê–ë–õ–ò–¶–ê: {header} ===\\n(—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {score:.3f})\\n{content}\")\n",
    "            \n",
    "        elif doc_type == \"image\":\n",
    "            for key, value in doc.metadata.get(\"links\", {}).items():\n",
    "                logger.info(f\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è -> {value}\")\n",
    "                if isinstance(value, str) and os.path.exists(value) and image_count < max_images and any(k in key.lower() for k in [\"—Ä–∏—Å—É–Ω–æ–∫\", \"–∫–∞—Ä—Ç–∞\"]):\n",
    "                    if value not in analyzed_images:\n",
    "                        base64_img = image_to_base64(value)\n",
    "                        if base64_img:\n",
    "                            instr = \"–û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –∏–∑–≤–ª–µ–∫–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –æ—Ç–Ω–æ—Å—è—â—É—é—Å—è –∫ –≤–æ–ø—Ä–æ—Å—É. –£–∫–∞–∂–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∑–æ–Ω—ã, –∑–Ω–∞—á–µ–Ω–∏—è, –∫–∞—Ç–µ–≥–æ—Ä–∏–∏).\"\n",
    "                            image_messages = [\n",
    "                                SystemMessage(content=instr),\n",
    "                                HumanMessage(content=[\n",
    "                                    {\"type\": \"text\", \"text\": f\"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (–≤–æ–∑–º–æ–∂–Ω–æ, –∫–∞—Ä—Ç—É –∏–ª–∏ —Ä–∏—Å—É–Ω–æ–∫), —É—á–∏—Ç—ã–≤–∞—è –≤–æ–ø—Ä–æ—Å: {question}\\n–ö–æ–Ω—Ç–µ–∫—Å—Ç: {header}\"},\n",
    "                                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/png;base64,{base64_img}\"}}\n",
    "                                ])\n",
    "                            ]\n",
    "                            \n",
    "                            try:\n",
    "                                image_response = llm.invoke(image_messages)\n",
    "                                logger.info(f\"–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è -> {value}: {image_response.content[:100]}...\")\n",
    "                                \n",
    "                                image_analysis_results.append(f\"{key.upper()} (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {score:.3f}): {image_response.content.strip()}\")\n",
    "                                analyzed_images.add(value)\n",
    "                                image_count += 1\n",
    "                            except Exception as e:\n",
    "                                logger.error(f\"rag_query Exception: image analysis -> {key}: {e}\")\n",
    "                                image_analysis_results.append(f\"{key.upper()}: [–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞]\")\n",
    "                                \n",
    "                        else:\n",
    "                            logger.error(f\"rag_query Error: Base64 not received for -> {value}\")\n",
    "                            image_analysis_results.append(f\"{key.upper()}: [–û—à–∏–±–∫–∞ base64]\")\n",
    "            image_parts.append(f\"=== –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ï: {header} ===\\n(—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {score:.3f})\\n{header}\")\n",
    "            \n",
    "        elif doc_type == \"formula\":\n",
    "            formula_parts.append(f\"=== –§–û–†–ú–£–õ–ê: {header} ===\\n(—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {score:.3f})\\n{doc.page_content}\")\n",
    "            \n",
    "        else:\n",
    "            truncated_content = doc.page_content[:2000] + \"...\" if len(doc.page_content) > 2000 else doc.page_content\n",
    "            text_parts.append(f\"=== –¢–ï–ö–°–¢: {header} ===\\n(—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {score:.3f})\\n{truncated_content}\")\n",
    "            \n",
    "    context_parts = (\n",
    "        [\"=== –ê–ù–ê–õ–ò–ó –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô ===\"] + image_analysis_results +\n",
    "        [\"=== –û–ü–ò–°–ê–ù–ò–Ø –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô ===\"] + image_parts +\n",
    "        [\"=== –¢–ê–ë–õ–ò–¶–´ ===\"] + table_parts +\n",
    "        [\"=== –§–û–†–ú–£–õ–´ ===\"] + formula_parts +\n",
    "        [\"=== –¢–ï–ö–°–¢ ===\"] + text_parts\n",
    "    )\n",
    "    \n",
    "    total_tokens = 0\n",
    "    final_context_parts = []\n",
    "    for part in context_parts:\n",
    "        part_tokens = len(part) // 4\n",
    "        \n",
    "        if total_tokens + part_tokens > MAX_TOKENS_PER_BATCH:\n",
    "            logger.warning(f\"–ü—Ä–µ–≤—ã—à–µ–Ω–∏–µ –ª–∏–º–∏—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤ ({MAX_TOKENS_PER_BATCH}). –ß–∞—Å—Ç—å –ø—Ä–æ–ø—É—â–µ–Ω–∞: {part[:50]}...\")\n",
    "            break\n",
    "            \n",
    "        final_context_parts.append(part)\n",
    "        total_tokens += part_tokens\n",
    "        \n",
    "    context = \"\\n\\n\".join(final_context_parts)\n",
    "    logger.info(f\"–§–∏–Ω–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: {len(context)} —Å–∏–º–≤–æ–ª–æ–≤, –æ—Ü–µ–Ω–æ—á–Ω–æ ~{total_tokens} —Ç–æ–∫–µ–Ω–æ–≤\")\n",
    "    \n",
    "    with open(\"context_log.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write(context)\n",
    "    logger.info(\"–ö–æ–Ω—Ç–µ–∫—Å—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ context_log.txt\")\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=(\n",
    "            \"–í—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º –∏ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º. \"\n",
    "            \"–û—Ç–≤–µ—á–∞–π —Ç–æ—á–Ω–æ, –∫—Ä–∞—Ç–∫–æ –∏ —Ç–æ–ª—å–∫–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. \"\n",
    "            \"–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –æ—Ç–¥–∞–≤–∞–π –¥–∞–Ω–Ω—ã–º –∏–∑ —Ç–∞–±–ª–∏—Ü –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –µ—Å–ª–∏ –æ–Ω–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∑–æ–Ω—ã, –≤–µ—Å–∞, –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã). \"\n",
    "            \"–ï—Å–ª–∏ –≤ —Ç–∞–±–ª–∏—Ü–µ –µ—Å—Ç—å —á–∏—Å–ª–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ, —è–≤–Ω–æ —É–∫–∞–∂–∏ –∏—Ö –≤ –æ—Ç–≤–µ—Ç–µ. \"\n",
    "            \"–£–∫–∞–∑—ã–≤–∞–π –∏—Å—Ç–æ—á–Ω–∏–∫–∏: —Ä–∞–∑–¥–µ–ª, —Ç–∞–±–ª–∏—Ü–∞, —Ä–∏—Å—É–Ω–æ–∫, —Ñ–æ—Ä–º—É–ª–∞, –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ. \"\n",
    "            \"–ï—Å–ª–∏ –∑–∞–ø—Ä–∞—à–∏–≤–∞–µ–º–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –Ω–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –≤—ã–≤–µ–¥–µ–Ω–∞ –∏–∑ —Ç–∞–±–ª–∏—Ü –∏–ª–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, —É–∫–∞–∂–∏ —ç—Ç–æ. \"\n",
    "            \"–ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç, –Ω–∞–ø–∏—à–∏ '–î–∞–Ω–Ω—ã–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç'. \"\n",
    "            f\"{' (–í–Ω–∏–º–∞–Ω–∏–µ: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω fallback, –¥–∞–Ω–Ω—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –Ω–µ–ø–æ–ª–Ω—ã–º–∏)' if fallback_used else ''}\"\n",
    "        )),\n",
    "        HumanMessage(content=(\n",
    "            f\"–ù–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ –∏ —Ç–æ—á–Ω–æ –Ω–∞ –≤–æ–ø—Ä–æ—Å. –£–∫–∞–∂–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏.\\n\"\n",
    "            f\"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\\n{context}\\n\\n\"\n",
    "            f\"–í–æ–ø—Ä–æ—Å: {question}\"\n",
    "        ))\n",
    "    ]\n",
    "    \n",
    "    messages_log = [{\"role\": msg.type, \"content\": str(msg.content)} for msg in messages]\n",
    "    with open(\"messages_log.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(messages_log, f, ensure_ascii=False, indent=2)\n",
    "    logger.info(\"–°–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è LLM —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ messages_log.json\")\n",
    "    \n",
    "    try:\n",
    "        logger.info(f\"–ó–∞–ø—Ä–æ—Å –∫ {MODEL_NAME}...\")\n",
    "        response = llm.invoke(messages)\n",
    "        logger.info(f\"–û—Ç–≤–µ—Ç –æ—Ç LLM: {response}\")\n",
    "        if response is None:\n",
    "            logger.error(\"LLM –≤–µ—Ä–Ω—É–ª None\")\n",
    "            raise ValueError(\"LLM –≤–µ—Ä–Ω—É–ª None\")\n",
    "        if not hasattr(response, 'content'):\n",
    "            logger.error(\"–û—Ç–≤–µ—Ç LLM –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∞—Ç—Ä–∏–±—É—Ç content\")\n",
    "            raise ValueError(\"–û—Ç–≤–µ—Ç LLM –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∞—Ç—Ä–∏–±—É—Ç content\")\n",
    "        if response.content is None:\n",
    "            logger.error(\"LLM –≤–µ—Ä–Ω—É–ª –æ—Ç–≤–µ—Ç —Å content=None\")\n",
    "            raise ValueError(\"LLM –≤–µ—Ä–Ω—É–ª –æ—Ç–≤–µ—Ç —Å content=None\")\n",
    "        answer = response.content.strip()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"rag_query Exception LLM: {e}, simplifying the query...\")\n",
    "        simple_msg = [HumanMessage(content=f\"–û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ: {question}. –ö–æ–Ω—Ç–µ–∫—Å—Ç: {context[:2000]}\")]\n",
    "        try:\n",
    "            response = llm.invoke(simple_msg)\n",
    "            logger.info(f\"–û—Ç–≤–µ—Ç –æ—Ç —É–ø—Ä–æ—â—ë–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞: {response}\")\n",
    "            \n",
    "            if response is None or not hasattr(response, 'content') or response.content is None:\n",
    "                logger.error(\"–£–ø—Ä–æ—â—ë–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å –≤–µ—Ä–Ω—É–ª None –∏–ª–∏ –æ—Ç–≤–µ—Ç –±–µ–∑ content\")\n",
    "                answer = \"–û—à–∏–±–∫–∞: LLM –Ω–µ –≤–µ—Ä–Ω—É–ª –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç\"\n",
    "            else:\n",
    "                answer = response.content.strip()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"–û—à–∏–±–∫–∞ —É–ø—Ä–æ—â—ë–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞: {e}\")\n",
    "            answer = \"–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏\"\n",
    "            \n",
    "    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤\n",
    "    sources = []\n",
    "    seen_files = defaultdict(list)\n",
    "    for doc, score in filtered_docs[:10]:\n",
    "        if doc.metadata.get(\"type\") in [\"table\", \"image\", \"formula\"]:\n",
    "            filename = doc.metadata.get(\"filename\", \"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–∞–π–ª\")\n",
    "            header = \", \".join(doc.metadata.get(\"headers\", [doc.metadata.get(\"header\", doc.metadata.get(\"source\", \"unknown\"))]))\n",
    "            seen_files[filename].append(f\"{header} ({doc.metadata.get('type')}, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {score:.3f})\")\n",
    "    \n",
    "    for filename, headers in seen_files.items():\n",
    "        sources.append(f\"{filename}: {'; '.join(headers)}\")\n",
    "    \n",
    "    if not sources:\n",
    "        sources.append(\"–ù–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–ª–∏ —Ñ–æ—Ä–º—É–ª\")\n",
    "    \n",
    "    full_answer = f\"{answer}\\n\\n–ò—Å—Ç–æ—á–Ω–∏–∫–∏:\\n\" + \"\\n\".join(f\"- {s}\" for s in sources)\n",
    "    rag_cache[norm_question] = full_answer\n",
    "    \n",
    "    with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(rag_cache, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "    return full_answer, sources\n",
    "\n",
    "def load_or_create_all(doc_path: str) -> Tuple[FAISS, Dict[str, str], bool]:\n",
    "    logger.info(\"–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "    \n",
    "    required_files = [INDEX_PATH, IMAGE_CACHE_FILE, CHUNKS_LOG_FILE, IMAGES_DIR, LINKS_DICT_FILE]\n",
    "    all_exist = all(os.path.exists(f) for f in required_files) and len(os.listdir(IMAGES_DIR)) > 0\n",
    "    if all_exist:\n",
    "        logger.info(\"‚úÖ –í—Å–µ –∫–µ—à–∏ –Ω–∞–π–¥–µ–Ω—ã. –ó–∞–≥—Ä—É–∂–∞—é –±–µ–∑ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏—è...\")\n",
    "        \n",
    "        with open(IMAGE_CACHE_FILE, 'r', encoding='utf-8') as f:\n",
    "            image_cache = json.load(f)\n",
    "            \n",
    "        with open(LINKS_DICT_FILE, 'r', encoding='utf-8') as f:\n",
    "            links = json.load(f)\n",
    "            \n",
    "        vectorstore = FAISS.load_local(INDEX_PATH, embeddings, allow_dangerous_deserialization=True)\n",
    "        logger.info(\"–ö–µ—à –∑–∞–≥—Ä—É–∂–µ–Ω.\")\n",
    "        return vectorstore, links, False\n",
    "    else:\n",
    "        logger.info(\"‚öôÔ∏è –ö–µ—à –Ω–µ –Ω–∞–π–¥–µ–Ω. –°–æ–∑–¥–∞—ë–º –≤—Å—ë –∑–∞–Ω–æ–≤–æ...\")\n",
    "        documents, links, fallback_used = extract_from_docx(doc_path)\n",
    "        vectorstore = build_index(documents)\n",
    "        return vectorstore, links, fallback_used\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    DOC_PATH = \"–°–ü 20.13330.2016 2024-09-05.docx\" # can be scaled to other files\n",
    "    logger.info(\"üöÄ –ó–∞–ø—É—Å–∫ RAG-—Å–∏—Å—Ç–µ–º—ã...\")\n",
    "    \n",
    "    try:\n",
    "        vectorstore, links, fallback_used = load_or_create_all(DOC_PATH)\n",
    "        questions = [\n",
    "            \"–í –∫–∞–∫–∏—Ö –∑–æ–Ω–∞—Ö –ø–æ –≤–µ—Å—É —Å–Ω–µ–≥–æ–≤–æ–≥–æ –ø–æ–∫—Ä–æ–≤–∞ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –•–µ—Ä—Å–æ–Ω –∏ –ú–µ–ª–∏—Ç–æ–ø–æ–ª—å?\",\n",
    "            \"–ö–∞–∫–∏–µ —Ä–µ–≥–∏–æ–Ω—ã –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏ –∏–º–µ—é—Ç –≤—ã—Å–æ—Ç–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç k_h, –ø—Ä–µ–≤—ã—à–∞—é—â–∏–π 2?\",\n",
    "            \"–ö–∞–∫–æ–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –ø–æ –Ω–∞–≥—Ä—É–∑–∫–µ –¥–ª—è –º–µ—Ç–∞–ª–ª–∏—á–µ—Å–∫–∏—Ö –∫–æ–Ω—Å—Ç—Ä—É–∫–∏—Ü–π?\",\n",
    "            \"–ö–∞–∫ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω–æ–π –≤–µ—Ç—Ä–æ–≤–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏ w?\",\n",
    "            \"–ß—Ç–æ —Ç–∞–∫–æ–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –ø–æ –Ω–∞–≥—Ä—É–∑–∫–µ?\",\n",
    "        ]\n",
    "        for q in questions:\n",
    "            print(\"\\n\" + \"=\"*77)\n",
    "            print(f\"*‚ùì* –í–æ–ø—Ä–æ—Å: {q}\")\n",
    "            print(\"-\" * 77)\n",
    "            full_answer, _ = rag_query(q, vectorstore, links, fallback_used=fallback_used)\n",
    "            print(full_answer)\n",
    "            print(\"=\"*77)\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"__main__ Exception -> critical: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49ebc43-b235-403d-9f00-a320d90ca726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bacd1aa2-020a-4288-b07a-c45518fed198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfecbf72-af0f-4679-9924-42ad3d12a5aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
